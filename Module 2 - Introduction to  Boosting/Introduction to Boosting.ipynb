{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1707245055614,"user":{"displayName":"Daniel Vasiliu","userId":"13280890113502714012"},"user_tz":300},"id":"0Zls1--d3vMC"},"outputs":[],"source":["# setup - graphical libraries\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","mpl.rcParams['figure.dpi'] = 120\n","from IPython.display import Image\n","from IPython.display import display"]},{"cell_type":"markdown","metadata":{"id":"ZdYECPVcuMdg"},"source":["# <font color='blue' size=8px> Boosting </font>\n","\n","Boosting (Schapire and Freund 2012) is a greedy algorithm for fitting adaptive basis-function where the weights are generated by an algorithm called a weak\n","learner or a base learner. The algorithm works by applying the weak learner sequentially to weighted versions of the data, where more weight is given to examples that were misclassified by earlier rounds.\n","This weak learner can be any classification or regression algorithm. In 1998, the late Leo Breiman called boosting, where the weak learner is a shallow\n","decision tree, the “best off-the-shelf classifier in the world” (Hastie et al. 2009, p340). (Reference: K. Murphy - \"Machine Learning - A Probilistic Perspective\", page 554.)\n","\n","\n","## How Does Adaptive Boosting Work?\n","\n","We can understand the working of the AdaBoost algorithm in step by step manner as going deep into the work, we can see there are multiple basic steps which this algorithm follows. Let’s take a look at these steps.\n","\n","**Step 1:** When the algorithm is given data, it starts by Assigning equal weights to all training examples in the dataset. These weights represent the importance of each sample during the training process.\n","\n","**Step 2:** Here, this algorithm iterates with a few algorithms for a specified number of iterations (or until a stopping criterion is met). The algorithm trains a weak classifier on the training data. Here the weak classifier can be considered a model that performs slightly better than random guessing, such as a decision stump (a one-level decision tree).\n","\n","**Step 3:** During each iteration, the algorithm trains the weak classifier on given training data with the current sample weights. The weak classifier aims to minimize the classification error, weighted by the sample weights.\n","\n","**Step 4:** After training the weak classifier, the algorithm calculates classifier weight based on the errors of the weak classifier. A weak classifier with a lower error receives a higher weight.\n","\n","**Step 4:** Once the calculation of weight completes, the algorithm updates sample weights, and the algorithm gives assigns higher weights to misclassified examples so that more importance in subsequent iterations can be given.\n","\n","**Step 5:** After updating the sample weights, they are normalized so that they sum up to 1 and Combine the predictions of all weak classifiers using a weighted majority vote. The weights of the weak classifiers are considered when making the final prediction.\n","\n","**Step 6:** Finally, Steps 2–5 are repeated for the specified number of iterations (or until the stopping criterion is met), with the sample weights updated at each iteration. The final prediction is obtained by aggregating the predictions of all weak classifiers based on their weights.\n","\n","The below pseudocode can be helpful in understanding the working of the AdaBoost algorithm.\n","\n","```pseudo\n","Initialize sample weights for each training example\n","For each iteration:\n","Train a weak classifier using the current sample weights\n","Calculate the error of the weak classifier\n","Calculate the weight of the weak classifier based on the error\n","Update the sample weights based on the weak classifier's performance\n","Normalize the sample weights\n","End the iterations\n","Combine the weak classifiers using a weighted majority vote.\n","```\n","\n","Reference: https://medium.com/@datasciencewizards/understanding-the-adaboost-algorithm-2e9344d83d9b"]},{"cell_type":"markdown","metadata":{"id":"c6dJdRiJ3_E-"},"source":["# <font color='blue' size=8px> Gradient Boosting </font>\n","\n","<font size=5px> Main Idea:\n","\n","Assume you have an regressor $F$ and, for the observation $x_i$ we make the prediction $F(x_i)$. To improve the predictions, we can regard $F$ as a 'weak learner' and therefore train a decision tree (we can call it $h$) where the new output is $y_i-F(x_i)$. So, the new predictor is trained on the residuals of the previous one. Thus, there are increased chances that the new regressor\n","\n","$$\\large F + h$$\n","\n","is better than the old one, $F.$\n","\n","Main task: implement this idea in an algorithm and test it on real data sets.\n","\n","\n","<figure>\n","<center>\n","<img src='https://i.imgur.com/K2RC0le.png'width='400px'/>\n","<figcaption>Computational Diagram for Gradient Boosting</figcaption></center>\n","</figure>\n","\n","\n","In gradient boosting for classification, the concept is very similar to the one used in regression, but instead of predicting residuals (numerical differences between predicted and actual values), we work with probabilities and classes.\n","\n","### 1. **Initialize with a Base Classifier:**\n","   \n","   The algorithm starts by making an initial prediction for each instance in the training data. This is usually a constant prediction, such as the mean of the target variable, but in classification, it might be based on class probabilities. For binary classification, this could mean predicting the probability of the positive class (e.g., 0.5 for a balanced dataset).\n","\n","### 2. **Compute Pseudo-Residuals:**\n","   \n","   Instead of computing residuals (differences between actual values and predicted values as in regression), we compute **pseudo-residuals**. In classification, this involves computing the difference between the true class label (often 0 or 1 for binary classification) and the predicted probability of the class.\n","\n","   For a binary classification problem, if the predicted probability for class 1 is $ p_i $ and the true label is $ y_i $, the pseudo-residual is:\n","   $$\n","   r_i = y_i - p_i\n","   $$\n","   In other words, it's the difference between the true label and the predicted probability.\n","\n","### 3. **Fit a New Weak Learner (Tree):**\n","   \n","   The algorithm then fits a new weak learner (usually a decision tree) to predict the pseudo-residuals. This tree is trained to focus on the areas where the current model performs poorly (i.e., where the errors or pseudo-residuals are large).\n","\n","### 4. **Update the Model:**\n","   \n","   The predictions from the new tree are used to update the overall model. Instead of directly adding these predictions to the existing model (as in regression), we adjust the predictions in a way that incorporates the probabilities and the contribution from the new tree. Specifically, in binary classification, we update the log-odds of the predicted class probabilities:\n","   $$\n","   \\text{logit}(p_i^{\\text{new}}) = \\text{logit}(p_i^{\\text{old}}) + \\alpha f(x_i)\n","   $$\n","   where $ f(x_i) $ is the prediction from the new tree, and $ \\alpha $ is the learning rate.\n","\n","   The updated predicted probability for class 1 is then:\n","   $$\n","   p_i^{\\text{new}} = \\frac{1}{1 + e^{-\\text{logit}(p_i^{\\text{new}})}}\n","   $$\n","\n","### 5. **Repeat the Process:**\n","   \n","   Steps 2 through 4 are repeated for a set number of iterations or until the model's performance converges. Each iteration adds a new tree that attempts to correct the errors of the previous trees.\n","\n","### 6. **Final Prediction:**\n","   \n","   After all iterations, the final prediction for each instance is typically the class with the highest probability. For binary classification, if the final probability $ p_i $ for class 1 is greater than 0.5, the predicted class is 1; otherwise, it’s 0.\n","\n","### For Multiclass Classification:\n","   In multiclass classification, the process is extended by using a one-vs-all or softmax approach to predict probabilities for each class. The algorithm works similarly, but pseudo-residuals are computed for each class, and the model is trained to improve predictions for all classes simultaneously.\n","\n","In summary, the key difference between regression and classification in gradient boosting is that, in classification, the model focuses on improving class probabilities, often in log-odds space, rather than directly predicting residuals as in regression. The weak learners (trees) are trained to reduce the classification error iteratively by focusing on pseudo-residuals related to the predicted probabilities."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"xrHdV08V4C13"},"outputs":[],"source":["# computational libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LinearRegression, Ridge\n","from sklearn.preprocessing import StandardScaler, QuantileTransformer, MinMaxScaler, PolynomialFeatures\n","from sklearn.decomposition import PCA\n","from scipy.spatial import Delaunay\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","import scipy.stats as stats\n","from sklearn.model_selection import train_test_split as tts, KFold, GridSearchCV\n","from sklearn.metrics import mean_squared_error as mse\n","from scipy.interpolate import interp1d, RegularGridInterpolator, griddata, LinearNDInterpolator, NearestNDInterpolator\n","from math import ceil\n","from scipy import linalg\n","# the following line(s) are necessary if you want to make SKlearn compliant functions\n","from sklearn.base import BaseEstimator, RegressorMixin\n","from sklearn.utils.validation import check_X_y, check_array, check_is_fitted"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"w5WbzbQHgjzN"},"outputs":[],"source":["scale = StandardScaler()"]},{"cell_type":"markdown","metadata":{"id":"M7U1TMXrgmIb"},"source":["## Kernels"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"2GSXLsUDgjiB"},"outputs":[],"source":["# Gaussian Kernel\n","def Gaussian(w):\n","  return np.where(w>4,0,1/(np.sqrt(2*np.pi))*np.exp(-1/2*w**2))\n","\n","# Tricubic Kernel\n","def Tricubic(w):\n","  return np.where(w>1,0,70/81*(1-w**3)**3)\n","\n","# Quartic Kernel\n","def Quartic(w):\n","  return np.where(w>1,0,15/16*(1-w**2)**2)\n","\n","# Epanechnikov Kernel\n","def Epanechnikov(w):\n","  return np.where(w>1,0,3/4*(1-w**2))"]},{"cell_type":"markdown","metadata":{"id":"bMa_Qu3s49Fn"},"source":["## Function Definitions\n","\n","We define all the useful functions we need.\n","\n","- we need a distance function\n","\n","- we need the locally weighted regression for predicting the train data\n","\n","- we need an encapsulation for SkLearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-rcevC3giH6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opnp4VRP5C9r"},"outputs":[],"source":["# here we have a function that computes the Euclidean distance between all the observations in u, and v\n","def dist(u,v):\n","  if len(v.shape)==1:\n","    v = v.reshape(1,-1)\n","  d = np.array([np.sqrt(np.sum((u-v[i])**2,axis=1)) for i in range(len(v))])\n","  return d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"efRF_1M75gLP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Osoez3m9XmV-"},"outputs":[],"source":["def lw_ag_md(x, y, xnew,f=2/3,iter=3, intercept=True):\n","\n","  n = len(x)\n","  r = int(ceil(f * n))\n","  yest = np.zeros(n)\n","\n","  if len(y.shape)==1: # here we make column vectors\n","    y = y.reshape(-1,1)\n","\n","  if len(x.shape)==1:\n","    x = x.reshape(-1,1)\n","\n","  if intercept:\n","    x1 = np.column_stack([np.ones((len(x),1)),x])\n","  else:\n","    x1 = x\n","\n","  h = [np.sort(np.sqrt(np.sum((x-x[i])**2,axis=1)))[r] for i in range(n)]\n","  # dist(x,x) is always symmetric\n","  w = np.clip(dist(x,x) / np.array(h), 0.0, 1.0)\n","  # note that w is a square matrix and in Python arithmetic operations such as\n","  # w**3 or 1-w**3 are performed element-wise\n","  #w = (1-w**3)**3 # a Tricubic kernel\n","  w = Epanechnikov(w)\n","\n","  #Looping through all X-points\n","  delta = np.ones(n)\n","  for iteration in range(iter):\n","    for i in range(n):\n","      W = np.diag(delta).dot(np.diag(w[i,:]))\n","      # when we multiply two diagonal matrices we get also a diagonal matrix\n","      b = np.transpose(x1).dot(W).dot(y)\n","      A = np.transpose(x1).dot(W).dot(x1)\n","      ##\n","      A = A + 0.0001*np.eye(x1.shape[1]) # if we want L2 regularization for solving the system\n","      beta = linalg.solve(A, b)\n","\n","      #beta, res, rnk, s = linalg.lstsq(A, b)\n","      yest[i] = np.dot(x1[i],beta.ravel())\n","\n","    residuals = y.ravel() - yest\n","    s = np.median(np.abs(residuals))\n","\n","    delta = np.clip(residuals / (6.0 * s), -1, 1)\n","\n","    delta = (1 - delta ** 2) ** 2\n","\n","  # here we are making predictions for xnew by using an interpolation and the predictions we made for the train data\n","  if x.shape[1]==1:\n","    f = interp1d(x.flatten(),yest,fill_value='extrapolate')\n","    output = f(xnew)\n","  else:\n","    output = np.zeros(len(xnew))\n","    for i in range(len(xnew)):\n","      ind = np.argsort(np.sqrt(np.sum((x-xnew[i])**2,axis=1)))[:r]\n","      pca = PCA(n_components=3)\n","      x_pca = pca.fit_transform(x[ind])\n","      tri = Delaunay(x_pca,qhull_options='QJ Pp')\n","      f = LinearNDInterpolator(tri,yest[ind])\n","      output[i] = f(pca.transform(xnew[i].reshape(1,-1)))\n","      # the output may have NaN's where the data points from xnew are outside the convex hull of X\n","\n","  if sum(np.isnan(output))>0:\n","    g = NearestNDInterpolator(x,yest.ravel())\n","    # output[np.isnan(output)] = g(X[np.isnan(output)])\n","    output[np.isnan(output)] = g(xnew[np.isnan(output)])\n","  return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-Rjqext6CZt"},"outputs":[],"source":["def lowess(x, y, xnew,kernel=Gaussian,tau=0.02,iter=1, intercept=True):\n","\n","  n = len(x)\n","\n","  yest = np.zeros(n)\n","\n","  if len(y.shape)==1: # here we make column vectors\n","    y = y.reshape(-1,1)\n","\n","  if len(x.shape)==1:\n","    x = x.reshape(-1,1)\n","\n","  if intercept:\n","    x1 = np.column_stack([np.ones((len(x),1)),x])\n","  else:\n","    x1 = x\n","\n","\n","  # dist(x,x) is always symmetric\n","  w = dist(x,x)\n","\n","\n","  #Looping through all X-points\n","  delta = np.ones(n)\n","  for iteration in range(iter):\n","    for i in range(n):\n","      W = np.diag(delta).dot(kernel(w[i,:]/(2*tau)).ravel())\n","      # when we multiply two diagonal matrices we get also a diagonal matrix\n","      b = np.transpose(x1).dot(np.diag(W)).dot(y)\n","      A = np.transpose(x1).dot(np.diag(W)).dot(x1)\n","      ##\n","      A = A + 0.0001*np.eye(x1.shape[1]) # if we want L2 regularization for solving the system\n","      beta = linalg.solve(A, b)\n","\n","      #beta, res, rnk, s = linalg.lstsq(A, b)\n","      yest[i] = np.dot(x1[i],beta.ravel())\n","\n","    residuals = y.ravel() - yest\n","    s = np.median(np.abs(residuals))\n","\n","    delta = np.clip(residuals / (6.0 * s), -1, 1)\n","\n","    delta = (1 - delta ** 2) ** 2\n","\n","  # here we are making predictions for xnew by using an interpolation and the predictions we made for the train data\n","  if x.shape[1]==1:\n","    f = interp1d(x.flatten(),yest,fill_value='extrapolate')\n","    output = f(xnew)\n","  else:\n","    output = np.zeros(len(xnew))\n","    for i in range(len(xnew)):\n","      w = np.diag(kernel(dist(x,xnew[i])/(2*tau)).ravel())\n","      # model = LinearRegression()\n","      # model.fit(w.dot(x),w.dot(yest))\n","      # output[i] = model.predict(xnew[i].reshape(1,-1))\n","\n","      output[i] = np.sum(w.dot(yest))/np.trace(w)\n","      # the output may have NaN's where the data points from xnew are outside the convex hull of X\n","\n","  if sum(np.isnan(output))>0:\n","    g = NearestNDInterpolator(x,yest.ravel())\n","    # output[np.isnan(output)] = g(X[np.isnan(output)])\n","    output[np.isnan(output)] = g(xnew[np.isnan(output)])\n","  return output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-2rhEhKPnbDi","outputId":"4abbfc00-d23f-4be4-b909-0eae09c53377"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-133-9dd72b8600c5>:57: RuntimeWarning: invalid value encountered in double_scalars\n","  output[i] = np.sum(w.dot(yest))/np.trace(w)\n"]},{"data":{"text/plain":["190.50550599947195"]},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":["# testing a different version of lowess\n","yhat = lowess(xtrain,ytrain,xtest,kernel=Epanechnikov,tau=0.2,iter=1)\n","mse(ytest,yhat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78RP1KJB6xgD"},"outputs":[],"source":["yhat2 = lw_ag_md(xtrain,ytrain,xtest,f=25/len(xtrain),iter=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cqccFe00nbAJ","outputId":"48a2435f-53af-4890-8c6d-2690aa4db63d"},"outputs":[{"data":{"text/plain":["59.17453576519482"]},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["mse(ytest,yhat2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9PAwoGLna9l"},"outputs":[],"source":["model = Lowess_AG_MD"]},{"cell_type":"markdown","metadata":{"id":"G0pZfYk1ffGy"},"source":["## Scikit-Learn Compliant Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6r_cVSa6dEPM"},"outputs":[],"source":["class Lowess_AG_MD:\n","    def __init__(self, f = 1/10, iter = 3,intercept=True):\n","        self.f = f\n","        self.iter = iter\n","        self.intercept = intercept\n","\n","    def fit(self, x, y):\n","        f = self.f\n","        iter = self.iter\n","        self.xtrain_ = x\n","        self.yhat_ = y\n","\n","    def predict(self, x_new):\n","        check_is_fitted(self)\n","        x = self.xtrain_\n","        y = self.yhat_\n","        f = self.f\n","        iter = self.iter\n","        intercept = self.intercept\n","        return lw_ag_md(x, y, x_new, f, iter, intercept) # this is actually our defined function of Lowess\n","\n","    def get_params(self, deep=True):\n","    # suppose this estimator has parameters \"f\", \"iter\" and \"intercept\"\n","        return {\"f\": self.f, \"iter\": self.iter,\"intercept\":self.intercept}\n","\n","    def set_params(self, **parameters):\n","        for parameter, value in parameters.items():\n","            setattr(self, parameter, value)\n","        return self"]},{"cell_type":"markdown","metadata":{"id":"0EJAtJ8zWZjj"},"source":["## The Boosted Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-paidTgFF1l"},"outputs":[],"source":["def boosted_lwr(x, y, xnew, f=1/3,iter=2,intercept=True):\n","  # we need decision trees\n","  # for training the boosted method we use x and y\n","  model1 = Lowess_AG_MD(f=f,iter=iter,intercept=intercept) # we need this for training the Decision Tree\n","  model1.fit(x,y)\n","  residuals1 = y - model1.predict(x)\n","  model2 = Lowess_AG_MD(f=f,iter=iter,intercept=intercept)\n","  #model2 = RandomForestRegressor(n_estimators=200,max_depth=9)\n","  model2.fit(x,residuals1)\n","  output = model1.predict(xnew) + model2.predict(xnew)\n","  return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ft-VEKIVFFrr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"UZ046CJLFFgw","outputId":"0afb8a6a-b87e-4668-bebc-5045c3d68021"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-918bdf85-5d1b-448f-95d7-3210f01c0173\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cement</th>\n","      <th>slag</th>\n","      <th>ash</th>\n","      <th>water</th>\n","      <th>superplastic</th>\n","      <th>coarseagg</th>\n","      <th>fineagg</th>\n","      <th>age</th>\n","      <th>strength</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>540.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>162.0</td>\n","      <td>2.5</td>\n","      <td>1040.0</td>\n","      <td>676.0</td>\n","      <td>28</td>\n","      <td>79.99</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>540.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>162.0</td>\n","      <td>2.5</td>\n","      <td>1055.0</td>\n","      <td>676.0</td>\n","      <td>28</td>\n","      <td>61.89</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>332.5</td>\n","      <td>142.5</td>\n","      <td>0.0</td>\n","      <td>228.0</td>\n","      <td>0.0</td>\n","      <td>932.0</td>\n","      <td>594.0</td>\n","      <td>270</td>\n","      <td>40.27</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>332.5</td>\n","      <td>142.5</td>\n","      <td>0.0</td>\n","      <td>228.0</td>\n","      <td>0.0</td>\n","      <td>932.0</td>\n","      <td>594.0</td>\n","      <td>365</td>\n","      <td>41.05</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>198.6</td>\n","      <td>132.4</td>\n","      <td>0.0</td>\n","      <td>192.0</td>\n","      <td>0.0</td>\n","      <td>978.4</td>\n","      <td>825.5</td>\n","      <td>360</td>\n","      <td>44.30</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1025</th>\n","      <td>276.4</td>\n","      <td>116.0</td>\n","      <td>90.3</td>\n","      <td>179.6</td>\n","      <td>8.9</td>\n","      <td>870.1</td>\n","      <td>768.3</td>\n","      <td>28</td>\n","      <td>44.28</td>\n","    </tr>\n","    <tr>\n","      <th>1026</th>\n","      <td>322.2</td>\n","      <td>0.0</td>\n","      <td>115.6</td>\n","      <td>196.0</td>\n","      <td>10.4</td>\n","      <td>817.9</td>\n","      <td>813.4</td>\n","      <td>28</td>\n","      <td>31.18</td>\n","    </tr>\n","    <tr>\n","      <th>1027</th>\n","      <td>148.5</td>\n","      <td>139.4</td>\n","      <td>108.6</td>\n","      <td>192.7</td>\n","      <td>6.1</td>\n","      <td>892.4</td>\n","      <td>780.0</td>\n","      <td>28</td>\n","      <td>23.70</td>\n","    </tr>\n","    <tr>\n","      <th>1028</th>\n","      <td>159.1</td>\n","      <td>186.7</td>\n","      <td>0.0</td>\n","      <td>175.6</td>\n","      <td>11.3</td>\n","      <td>989.6</td>\n","      <td>788.9</td>\n","      <td>28</td>\n","      <td>32.77</td>\n","    </tr>\n","    <tr>\n","      <th>1029</th>\n","      <td>260.9</td>\n","      <td>100.5</td>\n","      <td>78.3</td>\n","      <td>200.6</td>\n","      <td>8.6</td>\n","      <td>864.5</td>\n","      <td>761.5</td>\n","      <td>28</td>\n","      <td>32.40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1030 rows × 9 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-918bdf85-5d1b-448f-95d7-3210f01c0173')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-918bdf85-5d1b-448f-95d7-3210f01c0173 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-918bdf85-5d1b-448f-95d7-3210f01c0173');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      cement   slag    ash  water  superplastic  coarseagg  fineagg  age  \\\n","0      540.0    0.0    0.0  162.0           2.5     1040.0    676.0   28   \n","1      540.0    0.0    0.0  162.0           2.5     1055.0    676.0   28   \n","2      332.5  142.5    0.0  228.0           0.0      932.0    594.0  270   \n","3      332.5  142.5    0.0  228.0           0.0      932.0    594.0  365   \n","4      198.6  132.4    0.0  192.0           0.0      978.4    825.5  360   \n","...      ...    ...    ...    ...           ...        ...      ...  ...   \n","1025   276.4  116.0   90.3  179.6           8.9      870.1    768.3   28   \n","1026   322.2    0.0  115.6  196.0          10.4      817.9    813.4   28   \n","1027   148.5  139.4  108.6  192.7           6.1      892.4    780.0   28   \n","1028   159.1  186.7    0.0  175.6          11.3      989.6    788.9   28   \n","1029   260.9  100.5   78.3  200.6           8.6      864.5    761.5   28   \n","\n","      strength  \n","0        79.99  \n","1        61.89  \n","2        40.27  \n","3        41.05  \n","4        44.30  \n","...        ...  \n","1025     44.28  \n","1026     31.18  \n","1027     23.70  \n","1028     32.77  \n","1029     32.40  \n","\n","[1030 rows x 9 columns]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv('drive/MyDrive/Data Sets/concrete.csv')\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnAvtJn1Ya-M"},"outputs":[],"source":["x = data.loc[:,'cement':'age'].values\n","y = data['strength'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bih_bJbMYa5Q"},"outputs":[],"source":["xtrain, xtest, ytrain, ytest = tts(x,y,test_size=0.3,shuffle=True,random_state=123)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVQbYvBaYapX"},"outputs":[],"source":["xtrain = scale.fit_transform(xtrain)\n","xtest = scale.transform(xtest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QXq440kYt16"},"outputs":[],"source":["yhat = boosted_lwr(xtrain,ytrain,xtest,f=25/len(xtrain),iter=1,intercept=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fUh1YJorcS0F","outputId":"fc7bcf32-74c0-449c-efef-440efd46fb8f"},"outputs":[{"data":{"text/plain":["57.73151261936397"]},"execution_count":150,"metadata":{},"output_type":"execute_result"}],"source":["mse(ytest,yhat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1XmwFeucyJI"},"outputs":[],"source":["import xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVfpjE7Ec0mV"},"outputs":[],"source":["model_xgboost = xgboost.XGBRFRegressor(n_estimators=200,max_depth=7)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llGfQr_fdApG","outputId":"62c0f077-9c8d-4023-fbea-5d89e640c357"},"outputs":[{"data":{"text/plain":["32.63930815840025"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model_xgboost.fit(xtrain,ytrain)\n","mse(ytest,model_xgboost.predict(xtest))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXm2hx3cY2AY","outputId":"688f7557-a32c-4c73-8829-9784321a2e90"},"outputs":[{"data":{"text/plain":["57.73151261936397"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["mse(ytest,yhat)"]},{"cell_type":"markdown","metadata":{"id":"a3www91YkTOO"},"source":["## Test a Complete K-Fold CV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DcMO-O2vbBMC","outputId":"e7e2eb64-b02a-4762-bb05-c9ea9603c79b"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Cross-validated Mean Squared Error for Locally Weighted Regression is : 56.670304257102146\n","The Cross-validated Mean Squared Error for Random Forest is : 45.70628661801278\n"]}],"source":["mse_lwr = []\n","mse_rf = []\n","kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n","model_rf = RandomForestRegressor(n_estimators=200,max_depth=5)\n","\n","for idxtrain, idxtest in kf.split(x):\n","  xtrain = x[idxtrain]\n","  ytrain = y[idxtrain]\n","  ytest = y[idxtest]\n","  xtest = x[idxtest]\n","  xtrain = scale.fit_transform(xtrain)\n","  xtest = scale.transform(xtest)\n","\n","  yhat_lw = boosted_lwr(xtrain,ytrain,xtest,f=25/len(xtrain),iter=1,intercept=True)\n","\n","  model_rf.fit(xtrain,ytrain)\n","  yhat_rf = model_rf.predict(xtest)\n","\n","  mse_lwr.append(mse(ytest,yhat_lw))\n","  mse_rf.append(mse(ytest,yhat_rf))\n","print('The Cross-validated Mean Squared Error for Locally Weighted Regression is : '+str(np.mean(mse_lwr)))\n","print('The Cross-validated Mean Squared Error for Random Forest is : '+str(np.mean(mse_rf)))"]},{"cell_type":"markdown","metadata":{"id":"VZ6MQtWjXeTu"},"source":["## Polynomial Features\n","\n","This allows for more polynomially engineered features in the data. Let's see if results improve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUjqZiI17Dhe"},"outputs":[],"source":["poly = PolynomialFeatures(degree=2)\n","scale = StandardScaler()\n","pipe = Pipeline([['zscores',scale],['Poly',poly]])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9hL_GmGnXISW","outputId":"be6673f7-a283-40d0-9eda-e11a0ee2e6b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["MSE Fold 1 :63.80053774991302\n","MSE Fold 2 :47.73682100017316\n","MSE Fold 3 :43.12602069919888\n","MSE Fold 4 :37.937798704824715\n","MSE Fold 5 :38.87599271164445\n","MSE Fold 6 :44.95975550888706\n","MSE Fold 7 :57.703227331401564\n","MSE Fold 8 :56.35719574743018\n","MSE Fold 9 :88.60327031261076\n","MSE Fold 10 :58.216548891120496\n","The Cross-validated Mean Squared Error for Locally Weighted Regression is : 53.73171686572043\n"]}],"source":["mse_lwr = []\n","mse_rf = []\n","kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n","model_rf = RandomForestRegressor(n_estimators=200,max_depth=5)\n","i = 1\n","for idxtrain, idxtest in kf.split(x):\n","  xtrain = x[idxtrain]\n","  ytrain = y[idxtrain]\n","  ytest = y[idxtest]\n","  xtest = x[idxtest]\n","  xtrain = pipe.fit_transform(xtrain)\n","  xtest = pipe.transform(xtest)\n","\n","  yhat_lw = boosted_lwr(xtrain,ytrain,xtest,f=25/len(xtrain),iter=1,intercept=True)\n","\n","  # model_rf.fit(xtrain,ytrain)\n","  # yhat_rf = model_rf.predict(xtest)\n","\n","  mse_lwr.append(mse(ytest,yhat_lw))\n","  print('MSE Fold '+str(i)+' : '+str(mse(ytest,yhat_lw)))\n","  i += 1\n","  # mse_rf.append(mse(ytest,yhat_rf))\n","print('The Cross-validated Mean Squared Error for Locally Weighted Regression is : '+str(np.mean(mse_lwr)))\n","# print('The Cross-validated Mean Squared Error for Random Forest is : '+str(np.mean(mse_rf)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7ngdoTOZQnE"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"p13venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.0rc2"}},"nbformat":4,"nbformat_minor":0}
