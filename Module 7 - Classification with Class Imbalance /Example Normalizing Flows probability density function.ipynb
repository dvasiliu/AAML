{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/469], Loss: 339.2137\n",
      "Epoch [1/10], Step [200/469], Loss: 258.3534\n",
      "Epoch [1/10], Step [300/469], Loss: 224.7767\n",
      "Epoch [1/10], Step [400/469], Loss: 207.5470\n",
      "Epoch [2/10], Step [100/469], Loss: 200.9799\n",
      "Epoch [2/10], Step [200/469], Loss: 179.9624\n",
      "Epoch [2/10], Step [300/469], Loss: 178.0738\n",
      "Epoch [2/10], Step [400/469], Loss: 181.6971\n",
      "Epoch [3/10], Step [100/469], Loss: 165.3713\n",
      "Epoch [3/10], Step [200/469], Loss: 166.6384\n",
      "Epoch [3/10], Step [300/469], Loss: 156.0230\n",
      "Epoch [3/10], Step [400/469], Loss: 151.1497\n",
      "Epoch [4/10], Step [100/469], Loss: 162.5576\n",
      "Epoch [4/10], Step [200/469], Loss: 157.4131\n",
      "Epoch [4/10], Step [300/469], Loss: 166.3839\n",
      "Epoch [4/10], Step [400/469], Loss: 159.5267\n",
      "Epoch [5/10], Step [100/469], Loss: 153.9683\n",
      "Epoch [5/10], Step [200/469], Loss: 152.1700\n",
      "Epoch [5/10], Step [300/469], Loss: 157.0360\n",
      "Epoch [5/10], Step [400/469], Loss: 156.7315\n",
      "Epoch [6/10], Step [100/469], Loss: 145.7701\n",
      "Epoch [6/10], Step [200/469], Loss: 142.0586\n",
      "Epoch [6/10], Step [300/469], Loss: 152.6009\n",
      "Epoch [6/10], Step [400/469], Loss: 149.8780\n",
      "Epoch [7/10], Step [100/469], Loss: 140.4275\n",
      "Epoch [7/10], Step [200/469], Loss: 162.1433\n",
      "Epoch [7/10], Step [300/469], Loss: 151.5435\n",
      "Epoch [7/10], Step [400/469], Loss: 136.2950\n",
      "Epoch [8/10], Step [100/469], Loss: 153.5864\n",
      "Epoch [8/10], Step [200/469], Loss: 159.1390\n",
      "Epoch [8/10], Step [300/469], Loss: 146.2489\n",
      "Epoch [8/10], Step [400/469], Loss: 139.7397\n",
      "Epoch [9/10], Step [100/469], Loss: 129.1866\n",
      "Epoch [9/10], Step [200/469], Loss: 130.2255\n",
      "Epoch [9/10], Step [300/469], Loss: 138.6977\n",
      "Epoch [9/10], Step [400/469], Loss: 150.1612\n",
      "Epoch [10/10], Step [100/469], Loss: 141.3221\n",
      "Epoch [10/10], Step [200/469], Loss: 140.9178\n",
      "Epoch [10/10], Step [300/469], Loss: 141.7046\n",
      "Epoch [10/10], Step [400/469], Loss: 141.8361\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a simple Normalizing Flow model using RealNVP\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(RealNVP, self).__init__()\n",
    "\n",
    "        self.t1 = nn.Sequential(\n",
    "            nn.Linear(input_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim // 2),\n",
    "        )\n",
    "        self.s1 = nn.Sequential(\n",
    "            nn.Linear(input_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim // 2),\n",
    "            nn.Tanh(),  # To ensure scale is not too large\n",
    "        )\n",
    "\n",
    "        self.t2 = nn.Sequential(\n",
    "            nn.Linear(input_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim // 2),\n",
    "        )\n",
    "        self.s2 = nn.Sequential(\n",
    "            nn.Linear(input_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim // 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x[:, : x.shape[1] // 2], x[:, x.shape[1] // 2 :]\n",
    "\n",
    "        # First transformation\n",
    "        y1 = x1\n",
    "        y2 = x2 * torch.exp(self.s1(x1)) + self.t1(x1)\n",
    "\n",
    "        # Second transformation\n",
    "        z2 = y2\n",
    "        z1 = y1 * torch.exp(self.s2(y2)) + self.t2(y2)\n",
    "\n",
    "        z = torch.cat([z1, z2], dim=1)\n",
    "        return z\n",
    "\n",
    "    def inverse(self, z):\n",
    "        z1, z2 = z[:, : z.shape[1] // 2], z[:, z.shape[1] // 2 :]\n",
    "\n",
    "        # Inverse of second transformation\n",
    "        y2 = z2\n",
    "        y1 = (z1 - self.t2(y2)) * torch.exp(-self.s2(y2))\n",
    "\n",
    "        # Inverse of first transformation\n",
    "        x1 = y1\n",
    "        x2 = (y2 - self.t1(x1)) * torch.exp(-self.s1(x1))\n",
    "\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return x\n",
    "\n",
    "# Define the loss function (negative log-likelihood)\n",
    "def loss_function(z, log_det_J):\n",
    "    loss = 0.5 * torch.sum(z ** 2) - torch.sum(log_det_J)\n",
    "    return loss\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 784  # MNIST images are 28x28\n",
    "hidden_dim = 256\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the flow model and optimizer\n",
    "flow_model = RealNVP(input_dim, hidden_dim)\n",
    "optimizer = optim.Adam(flow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (data, _) in enumerate(train_loader):\n",
    "        # Flatten the image\n",
    "        data = data.view(-1, 784)\n",
    "\n",
    "        # Forward pass\n",
    "        z = flow_model(data)\n",
    "\n",
    "        # Calculate log determinant of Jacobian (not shown here for simplicity, \n",
    "        # but it involves tracking the scaling factors during the transformations)\n",
    "        # For this example, we'll assume log_det_J = 0\n",
    "        log_det_J = torch.zeros(data.shape[0]) \n",
    "\n",
    "        loss = loss_function(z, log_det_J)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-722.5181])\n"
     ]
    }
   ],
   "source": [
    "# Now you can use the trained flow model to estimate the probability density:\n",
    "def estimate_pdf(x, flow_model):\n",
    "    with torch.no_grad():\n",
    "        z = flow_model(x.view(-1, 784))\n",
    "        # Calculate the log probability density of z\n",
    "        log_p_z = -0.5 * torch.sum(z ** 2, dim=1) - (input_dim / 2) * torch.log(torch.tensor(2 * torch.pi))  \n",
    "        # If you need the actual probability, you can exponentiate log_p_z\n",
    "        # p_z = torch.exp(log_p_z)  \n",
    "        return log_p_z \n",
    "\n",
    "# Example usage:\n",
    "test_image = train_dataset[0][0]  # Get a test image from the dataset\n",
    "pdf_estimate = estimate_pdf(test_image, flow_model)\n",
    "print(pdf_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rockenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
